{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "657ea840-ca60-42f5-9bc1-a94bd5b0cd23",
   "metadata": {},
   "source": [
    "\n",
    "**Expanding the Capabilities of Language Models**\n",
    "\n",
    "In the realm of language models, we are witnessing a revolution marked by their ever-increasing fluency and sophistication. However, the journey doesn't end here. The true challenge lies in harnessing this fluency to develop language models that are not just eloquent, but also reliably capable assistants. This chapter delves into various strategies designed to imbue language models with enhanced intelligence, productivity, and trustworthiness.\n",
    "\n",
    "The overarching theme of our approach revolves around three core enhancements:\n",
    "\n",
    "1. **Utilizing Prompts Effectively**: We'll explore how carefully crafted prompts can significantly elevate the performance of language models. This isn't just about asking the right questions; it's about framing these questions in a way that guides the model towards more accurate and relevant responses.\n",
    "\n",
    "2. **Tool Integration**: By integrating external tools, we can compensate for the inherent limitations of language models, particularly in their world knowledge. This aspect covers how connecting to external data sources and services can enrich the model's responses, making them more grounded in reality and up-to-date.\n",
    "\n",
    "3. **Structured Reasoning Techniques**: We will delve into structured reasoning as a method to enhance the logical and analytical capabilities of language models. This involves teaching them to process information in a more organized and systematic manner, akin to how a skilled problem solver would approach a complex issue.\n",
    "\n",
    "Throughout this chapter, we will not only discuss these methods theoretically but also bring them to life through practical applications. Here's a sneak peek into what we'll cover:\n",
    "\n",
    "- **Combating Hallucinated Content**: A critical shortcoming of current language models is their tendency to produce hallucinated or inaccurate content. We will tackle this issue head-on by introducing automated fact-checking mechanisms. By cross-referencing the model's claims with available evidence, we aim to curb the spread of misinformation, ensuring that the information provided is not only fluent but also factually correct.\n",
    "\n",
    "- **Mastering Summarization**: A notable strength of language models lies in their ability to summarize content. We will investigate this capability further, examining how to enhance it through varying levels of prompt sophistication. Particularly for lengthy documents, we will introduce the concept of the map-reduce approach, a technique borrowed from computer science that helps in managing and summarizing extensive information efficiently.\n",
    "\n",
    "- **Extracting Information with Precision**: Moving forward, we will discuss how function calls can be used for extracting specific information from documents. This is a step towards more targeted and purposeful interactions with language models, where the focus is on retrieving precise data points rather than just general information.\n",
    "\n",
    "- **Application Development with Tool Integration**: To demonstrate the power of tool integration, we will develop an application that exemplifies how connecting to external data and services can greatly enhance the language model's utility, especially in compensating for its limited knowledge of the world.\n",
    "\n",
    "- **Applying Reasoning Strategies**: Finally, we will push the boundaries further by incorporating advanced reasoning strategies into our application. This will showcase how a language model, when equipped with the right tools and techniques, can not only process information but also reason through it in a more human-like manner.\n",
    "\n",
    "In summary, this chapter aims to transform the impressive fluency of language models into practical, reliable, and intelligent assistance. By bridging the gap between raw linguistic ability and applied intelligence, we're stepping into a future where language models become indispensable tools in our daily lives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc666296-4a79-4e9c-99eb-2ec4a1248a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMCheckerChain\n",
    "from langchain.llms import OpenAI\n",
    "from config2 import set_environment\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c546d2f-99e3-4a57-b99a-76d9a3e3bb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.284'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da44f0e3-577b-4ee5-9b19-c785b50f8683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMCheckerChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The platypus, a monotreme found in Australia, lays the biggest eggs of any mammal.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0.7, model=\"gpt-3.5-turbo-instruct\")\n",
    "text = \"What type of mammal lays the biggest eggs?\"\n",
    "checker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\n",
    "checker_chain.run(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f293945a-4481-49d5-8063-22d767b8aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Monotremes, a type of mammal found in Australia and parts of New Guinea, lay the largest eggs in the mammalian world. The eggs of the American echidna (spiny anteater) can grow as large as 10 cm in length, and dunnarts (mouse-sized marsupials found in Australia) can have eggs that exceed 5 cm in length.\n",
    "• Monotremes can be found in Australia and New Guinea\n",
    "• The largest eggs in the mammalian world are laid by monotremes\n",
    "• The American echidna lays eggs that can grow to 10 cm in length\n",
    "• Dunnarts lay eggs that can exceed 5 cm in length\n",
    "• Monotremes can be found in Australia and New Guinea – True\n",
    "• The largest eggs in the mammalian world are laid by monotremes – True\n",
    "• The American echidna lays eggs that can grow to 10 cm in length – False, the American echidna lays eggs that are usually between 1 to 4 cm in length.\n",
    "• Dunnarts lay eggs that can exceed 5 cm in length – False, dunnarts lay eggs that are typically between 2 to 3 cm in length.\n",
    "The largest eggs in the mammalian world are laid by monotremes, which can be found in Australia and New Guinea. Monotreme eggs can grow to 10 cm in length.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41281d0b-bbf0-4308-ba48-ef853c1aa066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMCheckerChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The statement is partially true. Monotremes are found in Australia and New Guinea and they do lay the largest eggs in the mammalian world. However, the specific measurements mentioned for the American echidna and dunnarts are false. The American echidna lays eggs that are usually between 1 to 4 cm in length and dunnarts lay eggs that are typically between 2 to 3 cm in length.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\n",
    "checker_chain.run(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a6e7425-5419-47a0-96bd-b4149b7e24d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "prompt = \"\"\"\n",
    "Summarize this text in one sentence:\n",
    "{text}\n",
    "\"\"\"\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "summary = llm(prompt.format(text=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a5aee2a-4407-4232-a5ba-0ea292bda5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4c1cd7f-aa59-4e9d-b878-2a15229e0fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_decorators import llm_prompt\n",
    "@llm_prompt\n",
    "def summarize(text:str, length=\"short\") -> str:\n",
    "    \"\"\"\n",
    "    Summarize this text in {length} length:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "    return\n",
    "summary = summarize(text=\"let me tell you a boring story from when I was young...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73c6e2b4-41fc-40b2-9345-e74fb6c05ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, OpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Summarize this text: {text}?\"\n",
    ")\n",
    "runnable = prompt | llm | StrOutputParser()\n",
    "summary = runnable.invoke({\"text\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ed19122-74e3-4cc6-bd0e-50be486f54bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The American echidna and dunnarts also lay eggs, but their eggs are smaller than those of monotremes, with the American echidna laying eggs that are usually between 1 to 4 cm in length and dunnarts laying eggs that are typically between 2 to 3 cm in length. '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b0918da-bdac-498f-a55d-7275bcc3a8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Article: { text }\n",
    "You will generate increasingly concise, entity-dense summaries of the above article.\n",
    "Repeat the following 2 steps 5 times.\n",
    "Step 1. Identify 1-3 informative entities (\";\" delimited) from the article which are missing from the previously generated summary.\n",
    "Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the missing entities.\n",
    "A missing entity is:\n",
    "- relevant to the main story,\n",
    "- specific yet concise (5 words or fewer),\n",
    "- novel (not in the previous summary),\n",
    "- faithful (present in the article),\n",
    "- anywhere (can be located anywhere in the article).\n",
    "Guidelines:\n",
    "- The first summary should be long (4-5 sentences, ~80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach ~80 words.\n",
    "- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n",
    "- Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n",
    "- The summaries should become highly dense and concise yet self-contained, i.e., easily understood without the article.\n",
    "- Missing entities can appear anywhere in the new summary.\n",
    "- Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n",
    "Remember, use the exact same number of words for each summary.\n",
    "Answer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are \"Missing_Entities\" and \"Denser_Summary\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e24ea2e7-3a35-4085-9f42-f51c19e464e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, OpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Summarize this text: {text}?\"\n",
    ")\n",
    "runnable = prompt | llm | StrOutputParser()\n",
    "summary = runnable.invoke({\"text\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cef9230f-7963-47f3-b7ba-d820f14419d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The American echidna lays eggs that can grow to 10 cm in length, but the average length is typically between 1 to 4 cm. Dunnarts, mouse-sized marsupials found in Australia, lay eggs that are usually between 2 to 3 cm in length.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca9c29c0-3a99-4e8c-a827-585d92852811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" This article discusses the effective use of Matplotlib for data visualization in Python, including code examples and a link to a tutorial for further learning. It also discusses the author's personal experience with Matplotlib and its benefits for practical business purposes, such as creating bar charts and customizing plots. The article also includes discussions on using other tools like pandas and seaborn, and addresses potential challenges and confusion for new users. Overall, the article provides a comprehensive guide for effectively using Matplotlib in data analysis.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain import OpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "pdf_file_path = \"matplotlib.pdf\"\n",
    "pdf_loader = PyPDFLoader(pdf_file_path)\n",
    "docs = pdf_loader.load_and_split()\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e870831-3e50-4418-8b60-325c3e63f7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Why was the light bulb feeling so bright?\n",
      "Because it had finally found its filament!\n",
      "Total Tokens: 26\n",
      "Prompt Tokens: 8\n",
      "Completion Tokens: 18\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI, PromptTemplate\n",
    "from langchain.callbacks import get_openai_callback\n",
    "llm_chain = PromptTemplate.from_template(\"Tell me a joke about {topic}!\") | OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "with get_openai_callback() as cb:\n",
    "    response = llm_chain.invoke(dict(topic=\"light bulbs\"))\n",
    "    print(response)\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13a57121-9565-4afd-9c0b-9bf8eec1fa81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\nWhy did the sock go to the doctor?\\n\\nBecause he was feeling a little un-well!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the computer go to the doctor?\\n\\nBecause it had a virus!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy was the shoe arrested?\\n\\nBecause it was laced! ', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 71, 'prompt_tokens': 21, 'completion_tokens': 50}, 'model_name': 'gpt-3.5-turbo-instruct'}, run=[RunInfo(run_id=UUID('d25cf514-5f65-4e56-9679-ca81a4c71195')), RunInfo(run_id=UUID('fba0613d-d95a-431d-b0e1-4202aa5801ed')), RunInfo(run_id=UUID('03233509-b279-43e8-8de9-7e7838ba89e9'))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "input_list = [\n",
    "    {\"topic\": \"socks\"},\n",
    "    {\"topic\": \"computers\"},\n",
    "    {\"topic\": \"shoes\"}\n",
    "]\n",
    "LLMChain(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo-instruct\"), \n",
    "    prompt=PromptTemplate.from_template(\"Tell me a joke about {topic}!\")\n",
    ").generate(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39873d9d-75c8-48a1-a0b0-6640b5cf4e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-3.5-turbo-0613',\n",
       " 'object': 'chat.completion',\n",
       " 'usage': {'completion_tokens': 17, 'prompt_tokens': 57, 'total_tokens': 74}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " {\n",
    "  \"model\": \"gpt-3.5-turbo-0613\",\n",
    "  \"object\": \"chat.completion\",\n",
    "  \"usage\": {\n",
    "    \"completion_tokens\": 17,\n",
    "    \"prompt_tokens\": 57,\n",
    "    \"total_tokens\": 74\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2665b0cf-ecb9-4734-9f89-3dce6b66535d",
   "metadata": {},
   "source": [
    "In June 2023, OpenAI unveiled enhancements to its API, introducing a transformative feature: function calling. This upgrade, a natural progression from instruction tuning, empowers developers to leverage OpenAI's models for structured data extraction. Imagine directing an AI to meticulously parse text and return neatly organized information in a specific JSON format. That's the prowess of this feature.\n",
    "\n",
    "Consider the task of sifting through documents to extract key entities. Developers can now instruct OpenAI's gpt-4-0613 and gpt-3.5-turbo-0613 models to produce a JSON object containing function arguments. This functionality bridges the gap between GPT models and external tools or APIs, streamlining the retrieval of structured data.\n",
    "\n",
    "Let's delve into the technicalities. The API's /v1/chat/completions endpoint has been augmented with a new parameter: functions. This parameter is a concoction of the function's name, a descriptive narrative, its parameters, and the function's core. By crafting functions in JSON schema, developers can guide the model to output precisely formatted data.\n",
    "\n",
    "LangChain harnesses this capability for two primary purposes: information extraction and plugin interfacing. For instance, in extracting data from a document, LangChain can engage OpenAI chat models to pinpoint specific entities and their attributes. This method shines in scenarios like analyzing a Curriculum Vitae (CV) to identify mentioned individuals, ensuring the output adheres to a predetermined structure.\n",
    "\n",
    "The beauty of this approach lies in its precision. Developers can stipulate the exact attributes they need, differentiating between mandatory and optional properties. The resulting schema is typically a dictionary, but for added sophistication, Pydantic can be employed. This popular parsing library offers enhanced control and flexibility, perfectly complementing the extraction process.\n",
    "\n",
    "To sum up, OpenAI's function calling feature is a game-changer in the realm of AI-assisted data extraction, offering developers a robust toolkit to extract and structure information with unprecedented accuracy and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47e7e599-b376-45e5-a8ef-edc72d8b5fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel\n",
    "class Experience(BaseModel):\n",
    "    start_date: Optional[str]\n",
    "    end_date: Optional[str]\n",
    "    description: Optional[str]\n",
    "class Study(Experience):\n",
    "    degree: Optional[str]\n",
    "    university: Optional[str]\n",
    "    country: Optional[str]\n",
    "    grade: Optional[str]\n",
    "class WorkExperience(Experience):\n",
    "    company: str\n",
    "    job_title: str\n",
    "class Resume(BaseModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    linkedin_url: Optional[str]\n",
    "    email_address: Optional[str]\n",
    "    nationality: Optional[str]\n",
    "    skill: Optional[str]\n",
    "    study: Optional[Study]\n",
    "    work_experience: Optional[WorkExperience]\n",
    "    hobby: Optional[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16012435-f02f-4eac-bc97-f70240fca2a3",
   "metadata": {},
   "source": [
    "1. **pdf_file_path Variable**: This refers to a variable in your code that stores the path to a PDF file. This path can be either relative or absolute:\n",
    "   \n",
    "   - **Relative Path**: A relative path refers to the location of the PDF file in relation to the current working directory of your script or application. For example, if your script is in a folder named `scripts`, and the PDF is in a folder named `documents` at the same level as `scripts`, the relative path from your script to the PDF might be `../documents/myfile.pdf`.\n",
    "   - **Absolute Path**: An absolute path, on the other hand, is the full path to a file regardless of the current working directory. It typically starts with the root directory. For example, on a Windows system, it could be `C:\\Users\\Username\\Documents\\myfile.pdf`, or on a Unix-like system, `/home/username/documents/myfile.pdf`.\n",
    "\n",
    "2. **Expected Output**: The statement implies that when the correct path to the PDF file is provided to the variable `pdf_file_path`, your code should process the PDF and produce a specific output. The nature of this output isn't specified in your statement, but it could be anything from extracting text, images, or data from the PDF, to performing some kind of analysis or transformation on the contents of the PDF.\n",
    "\n",
    "3. **Handling the Path**: In your code, you would use the `pdf_file_path` variable to access the PDF file. The way you handle this path depends on what you're doing with the PDF. For instance, if you're using a library to read or manipulate the PDF, you'd pass this path as an argument to the appropriate function or method provided by the library.\n",
    "\n",
    "In summary, `pdf_file_path` is a crucial piece in your code that points to the location of a PDF file you want to work with. Ensuring that this path is correctly set is key to successfully processing the file and obtaining the desired output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50d71c19-cd94-4aa2-962b-165e637e591a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Resume(first_name='RICKY', last_name='SAMBO MACHARM', linkedin_url='https://www.linkedin.com/in/theafricanquant', email_address='ricky.macharm@sisengai.com', nationality='Nigeria', skill='MongoDB', study=Study(start_date='2021', end_date='', description='Master of Science in Financial Engineering', degree='', university='World Quant University', country='Germany', grade=''), work_experience=WorkExperience(start_date='2022', end_date='current', description='Data Science Teaching Assistant (Remote) at WorldQuant University', company='', job_title=''), hobby='')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_extraction_chain_pydantic\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "pdf_file_path = 'Ricky_Data Scientist_2023.pdf'\n",
    "pdf_loader = PyPDFLoader(pdf_file_path)\n",
    "docs = pdf_loader.load_and_split()\n",
    "# please note that function calling is not enabled for all models!\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0613\")\n",
    "chain = create_extraction_chain_pydantic(pydantic_schema=Resume, llm=llm)\n",
    "chain.run(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3017941c-5869-445a-aaf6-58e3d64a86f2",
   "metadata": {},
   "source": [
    "1. **Function Calls in OpenAI Models**: Think of \"function calls\" as special instructions that you can give to OpenAI's language models. These instructions are formatted in a specific way (a syntax) that the model understands. When you include these function calls in your input to the model, the model can perform specific actions or tasks based on those instructions. However, since these instructions take up space in the model's input (known as the context), they count towards the limit of how much information you can send to the model at once. Additionally, you're billed for these function calls as if they were part of your regular input.\n",
    "\n",
    "2. **LangChain and Function Calls**: LangChain is a tool that can work with language models. One of its features is that it can automatically create these function calls and include them in prompts (the input text you send to the model). The cool part is that LangChain isn't limited to just OpenAI models; it can use other providers’ models for this purpose too. This makes LangChain a versatile tool in building applications that leverage language models.\n",
    "\n",
    "3. **Interactive Web App with Streamlit**: Streamlit is a tool that allows you to quickly create web applications. The idea here is to use Streamlit to build an interactive web application that utilizes these advanced language model features, like function calling, through LangChain.\n",
    "\n",
    "4. **Instruction Tuning and Function Calling**: These are techniques to make language models (like GPT) do specific tasks or produce specific types of outputs, like generating code that can be executed. It's like teaching the model to understand and respond to very specific types of requests.\n",
    "\n",
    "5. **Tool Integrations and Live Data Connections**: By using instruction tuning and function calls, language models can be integrated with other tools and services. For example, a language model could be used to fetch live data from an external source or interact with different software services. This makes the language model not just a text generator but a part of a larger, interactive system.\n",
    "\n",
    "6. **Augmenting Context with External Knowledge Sources**: Finally, tools can be used to enhance the language model's understanding by providing it with information from external sources. This is like giving the model access to a library of information that it can use to better understand and respond to queries.\n",
    "\n",
    "Function calling in the context of large language models (LLMs) such as GPT-3.5-turbo-0613 and GPT-4-0613 refers to a feature that enables developers to define functions using a JSON schema and guide the model to invoke those functions. The model then creates a JSON object encapsulating the arguments needed for the function call, which developers can leverage in their code to interact with APIs or external tools. This capability allows LLMs to extract structured data from unstructured text, enabling tasks such as creating intelligent chatbots, converting natural language into API calls, and extracting structured data from text. Function calling is seen as a channel for more reliable and structured connections between LLMs and external tools and services, offering developers a robust tool to connect external APIs and services in a more reliable and structured fashion[1].\r\n",
    "\r\n",
    "The rise of open-source, commercially permissive LLMs is revolutionizing generative AI, presenting organizations with enhanced control, minimized data risks, and cost benefits compared to proprietary models. Function calling capabilities are being evaluated and developed in LLMs such as NexusRaven-13B, which is tailored for function calling in operating software tools, enabling high-quality function-calling models for various applications[3].\r\n",
    "\r\n",
    "Function calling is considered a transformative feature that significantly broadens the capabilities of LLMs, allowing them to go beyond basic text generation and language understanding. It enables models to execute functions and APIs from natural language prompts, thereby orchestrating increasingly complex chains of external services. This makes the technique valuable across many real-world applications, and it is expected that most major AI providers will incorporate function calling into their models[4].\r\n",
    "\r\n",
    "In summary, function calling in LLMs empowers developers to define functions, which the model can then use to generate arguments for those functions, enabling the extraction of structured data from unstructured text and the orchestration of complex chains of external services through natural language prompts[1][4].\r\n",
    "\r\n",
    "Citations:\r\n",
    "[1] https://www.softude.com/blog/function-calling-in-open-ai-language-models\r\n",
    "[2] https://crunchingthedata.com/when-to-use-function-calling-for-llms/\r\n",
    "[3] https://openreview.net/pdf?id=5lcPe6DqfI\r\n",
    "[4] https://gradientflow.com/expanding-ai-horizons-the-rise-of-function-calling-in-llms/\r\n",
    "[5] https://arxiv.org/abs/2310.15213"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807958d8-70f1-4224-a24e-5c244027adad",
   "metadata": {},
   "source": [
    "Function calling in large language models (LLMs) improves performance by enabling the models to interact with external tools, APIs, and services, thereby expanding their capabilities beyond basic text generation and language understanding. This feature allows LLMs to translate natural language inputs into structured formats, trigger function calls within complex systems, and then translate the results back into natural language for user consumption. By leveraging function calling, LLMs can act as interactive interfaces, execute tasks, and access external services, leading to the creation of more adaptable and dynamic AI systems[2][5].\n",
    "\n",
    "While function calling may not directly improve the veracity or predictive performance of the information contained in the output itself, it does enhance the models' ability to produce appropriately formatted and structured outputs. This can be particularly valuable in scenarios where the goal is to extract structured data from unstructured text, create intelligent chatbots, or orchestrate complex chains of external services through natural language prompts[1][5].\n",
    "\n",
    "Additionally, the integration of function calling into LLMs allows them to serve crucial roles in real-world business scenarios, such as operating software, by providing a high degree of reliability and accuracy while keeping costs low[4]. Therefore, function calling is a transformative feature that significantly broadens the capabilities of LLMs, paving the way for the next wave of intelligent applications[5].\n",
    "\n",
    "Citations:\n",
    "[1] https://crunchingthedata.com/when-to-use-function-calling-for-llms/\n",
    "[2] https://www.linkedin.com/pulse/decoding-function-calling-redefining-boundaries-llm-application-jha\n",
    "[3] https://arize.com/blog/calling-all-functions-benchmarking-openai-function-calling-and-explanations/\n",
    "[4] https://openreview.net/pdf?id=5lcPe6DqfI\n",
    "[5] https://gradientflow.com/expanding-ai-horizons-the-rise-of-function-calling-in-llms/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53001e7b-6eb1-4d31-8b23-ed7dcd6a2f2d",
   "metadata": {},
   "source": [
    "## Real-World Applications of Large Language Models (LLMs) with Function Calling\n",
    "\n",
    "Some real-world applications of large language models (LLMs) with function calling include:\n",
    "\n",
    "1. **Conversational Agents and Chatbots**: LLMs power advanced chatbots and virtual assistants that engage in natural and meaningful conversations. [^2]\n",
    "2. **Content Generation**: LLMs are capable of generating high-quality content, including articles, blog posts, social media posts, and more. They can assist content creators by suggesting ideas, writing drafts, and even auto-completing sentences. [^2]\n",
    "3. **Language Translation**: LLMs have enabled significant advancements in machine translation. [^2]\n",
    "4. **Medicine**: LLMs are used to answer medical questions, extract information, manage health records, and even aid in designing new drugs and spotting diseases. [^2]\n",
    "5. **Robotics**: LLMs have found applications in robotics, aiding in task planning and automation. [^2]\n",
    "6. **Operating Software**: LLMs designed for function calling can be applied in real-world business scenarios to serve a crucial role in the common task of operating software, demanding a high degree of reliability and accuracy while keeping costs low. [^5]\n",
    "\n",
    "Function calling in LLMs has the potential to democratize access to complex systems, enhance user experience and productivity, and reshape how various tasks are performed in diverse domains. [^4]\n",
    "\n",
    "### Citations\n",
    "\n",
    "[^1]: [When to Use Function Calling for LLMs](https://crunchingthedata.com/when-to-use-function-calling-for-llms/)\n",
    "[^2]: [Large Language Models and Their Applications](https://www.labellerr.com/blog/large-language-models-and-their-applications/)\n",
    "[^3]: [Expanding AI Horizons: The Rise of Function Calling in LLMs](https://gradientflow.com/expanding-ai-horizons-the-rise-of-function-calling-in-llms/)\n",
    "[^4]: [Decoding Function Calling: Redefining Boundaries of LLM Application](https://www.linkedin.com/pulse/decoding-function-calling-redefining-boundaries-llm-application-jha)\n",
    "[^5]: [OpenReview Article on LLMs](https://openreview.net/pdf?id=5lcPe6DqfI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b449891-50e2-49a9-a53e-7a1df3feb453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import (\n",
    "    AgentExecutor, AgentType, initialize_agent, load_tools\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "def load_agent() -> AgentExecutor:\n",
    "    llm = ChatOpenAI(temperature=0, streaming=True)\n",
    "    # DuckDuckGoSearchRun, wolfram alpha, arxiv search, wikipedia\n",
    "    # TODO: try wolfram-alpha!\n",
    "    tools = load_tools(\n",
    "        tool_names=[\"ddg-search\", \"wolfram-alpha\", \"arxiv\", \"wikipedia\"],\n",
    "        llm=llm\n",
    "    )\n",
    "    return initialize_agent(\n",
    "        tools=tools, llm=llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2d30e10-62ac-4783-8b10-ef606b7bef34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 10:55:38.964 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /home/ricky/mambaforge/envs/langchain_ai/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from langchain.callbacks import StreamlitCallbackHandler\n",
    "chain = load_agent()\n",
    "st_callback = StreamlitCallbackHandler(st.container())\n",
    "if prompt := st.chat_input():\n",
    "    st.chat_message(\"user\").write(prompt)\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        st_callback = StreamlitCallbackHandler(st.container())\n",
    "        response = chain.run(prompt, callbacks=[st_callback])\n",
    "        st.write(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90627702-3065-4777-bdb8-263f3320f2c5",
   "metadata": {},
   "source": [
    "1. **Summarizing Documents with LLMs**: Large Language Models (LLMs) can be used to summarize documents by understanding and synthesizing the text. Two common approaches for this are \"stuffing\" all documents into a single prompt or using a map-reduce approach where each document is summarized individually and then the summaries are combined into a final summary. The choice of LLM for summarization can depend on the specific task and the length of the documents. For instance, RoBERTa has been suggested to outperform GPT on summarization tasks[1][5][9].\r\n",
    "\r\n",
    "2. **Chain of Density**: The \"Chain of Density\" is a method used in LLMs for summarization. It aims to produce highly dense and concise summaries that are self-contained and easily understood without the original document[6][10].\r\n",
    "\r\n",
    "3. **LangChain Decorators and LangChain Expression Language**: In LangChain, decorators like `@chain` can be used to turn an arbitrary function into a chain, which is functionally equivalent to wrapping in a RunnableLambda. This improves observability by tracing the chain correctly. The LangChain Expression Language allows users to compose arbitrary sequences together and get several benefits. It is used for creating new chains, editing steps, and exposing streaming, batch, and async interfaces[3][7][11][15].\r\n",
    "\r\n",
    "4. **Map-Reduce in LangChain**: MapReduce is a programming model used for processing large data sets in parallel across a distributed cluster or grid. In LangChain, it consists of two main steps: a \"map\" step where each document is summarized individually, and a \"reduce\" step where the summaries are combined into a final summary[4][5][12].\r\n",
    "\r\n",
    "5. **Counting Tokens**: Counting tokens in LLMs is important for managing the model's computational resources and ensuring that the input does not exceed the model's maximum token limit. The token count can affect the model's performance and the quality of the output.\r\n",
    "\r\n",
    "6. **Instruction Tuning and Function Calling**: Instruction tuning is a method used to improve the performance of LLMs by refining the instructions given to the model. Function calling, on the other hand, allows LLMs to interact with external tools and services. Both techniques can enhance the capabilities of LLMs and improve their performance in different tasks.\r\n",
    "\r\n",
    "7. **Tools in LangChain**: LangChain provides various tools for building and managing LLM applications. These include decorators for creating chains, the LangChain Expression Language for composing sequences, and debugging tools for inspecting the behavior of chains[7][15].\r\n",
    "\r\n",
    "8. **Agent Paradigms**: Agent paradigms refer to the different ways in which AI agents can be designed and used. For example, one paradigm might focus on using agents to interact with users in a conversational manner, while another might use agents to analyze and summarize large amounts of text.\r\n",
    "\r\n",
    "9. **Streamlit**: Streamlit is an open-source Python library that allows you to create custom web apps for machine learning and data science projects. It is designed to help data scientists and machine learning engineers to create interactive web applications quickly and easily.\r\n",
    "\r\n",
    "10. **Automated Fact-Checking**: Automated fact-checking involves using AI and machine learning techniques to verify the accuracy of information. This can involve comparing the information against a database of known facts, using natural language processing to understand the context of the information, and using machine learning algorithms to predict the likelihood of the information being true.\r\n",
    "\r\n",
    "Citations:\r\n",
    "[1] https://www.reddit.com/r/LocalLLaMA/comments/1891o5m/whats_the_best_llm_for_summarization_of_long/\r\n",
    "[2] https://smith.langchain.com/hub/langchain-ai/chain-of-density\r\n",
    "[3] https://python.langchain.com/docs/expression_language/how_to/decorator\r\n",
    "[4] https://api.python.langchain.com/en/latest/chains/langchain.chains.mapreduce.MapReduceChain.html\r\n",
    "[5] https://python.langchain.com/docs/use_cases/summarization\r\n",
    "[6] https://www.reddit.com/r/LangChain/comments/16mv84c/from_sparse_to_dense_gpt4_summarization_with/\r\n",
    "[7] https://blog.langchain.dev/the-new-langchain-architecture-langchain-core-v0-1-langchain-community-and-a-path-to-langchain-v0-1/\r\n",
    "[8] https://www.youtube.com/watch?v=OTL4CvDFlro\r\n",
    "[9] https://news.ycombinator.com/item?id=37946023\r\n",
    "[10] https://twitter.com/LangChainAI/status/1712115247530848492\r\n",
    "[11] https://python.langchain.com/docs/expression_language/how_to/\r\n",
    "[12] https://www.reddit.com/r/LangChain/comments/18s064m/how_does_mapreduce_work/\r\n",
    "[13] https://cohere.com/summarize\r\n",
    "[14] https://smith.langchain.com/hub/lawwu/chain_of_density?ref=blog.langchain.dev\r\n",
    "[15] https://blog.finxter.com/python-langchain-course-%F0%9F%90%8D%F0%9F%A6%9C%F0%9F%94%97-rci-and-langchain-expression-language-6-6/\r\n",
    "[16] https://stackoverflow.com/questions/76396514/how-can-i-use-the-map-reduce-chain-instead-of-the-stuff-chain-in-my-conversati\r\n",
    "[17] https://www.linkedin.com/pulse/very-long-discussion-legal-document-summarization-using-leonard-park\r\n",
    "[18] https://www.linkedin.com/pulse/art-science-summarization-unpacking-chain-density-rajaratnam\r\n",
    "[19] https://github.com/langchain-ai/langchain/issues/16643\r\n",
    "[20] https://community.openai.com/t/langchain-improve-prompt-latency-with-map-reduce/447774\r\n",
    "[21] https://arxiv.org/abs/2305.14239\r\n",
    "[22] https://twitter.com/LangChainAI/status/1705663573186216119\r\n",
    "[23] https://www.python-engineer.com/posts/langchain-crash-course/\r\n",
    "[24] https://www.reddit.com/r/LangChain/comments/165xmzx/ive_been_exploring_the_best_way_to_summarize/\r\n",
    "[25] https://www.assemblyai.com/blog/automatic-summarization-llms-python/net/pdf?id=5lcPe6DqfIng-in-llms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daf2c2b-ecdb-48a1-a293-52c2b30e916d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
